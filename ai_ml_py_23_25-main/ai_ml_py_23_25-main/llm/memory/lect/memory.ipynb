{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Memorie di un LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La \"memoria\" è quella parte di un'applicazione che permette di attributire uno \"stato\" ad una conversazione con un llm.\n",
    "\n",
    "Il processo di gestione della memoria si inserisce in modo piuttosto immediato all'interno della pipeline di una chat.\n",
    "\n",
    "1. Ad ogni nuovo inserimento di testo, le funzionalità di \"retrieving\" aggiungono lo storico al prompt da passare all'llm.\n",
    "2. L'llm elabora la risposta.\n",
    "3. Le funzionalità di storage conservano il nuovo scambio \"AI-User\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'elemento che varia tra le applicazioni è il livello di complessità con il quale si vuole gestire la memoria. A grandi linee, gli approcci individuabili sono tre:\n",
    "\n",
    "1. **memoria lineare:** si conservano e rcuperano un numero massimo di n-scambi\n",
    "2. **memoria non lineare:** l'implementazione dipende dalla dimensione dei dati conservati o  dall'importanza da assegnare a determinate parole o frasi\n",
    "Nel primo caso avremo una struttura dati che conserva e restituisce una sintesi delle conversazioni. Mentre, nel secondo caso avremo una struttra che conserva l'intera conversazione ma ne restituisce un campionamento, guidato dal testo corrente inserito da un utente\n",
    "3. **memoria persistente:** la memoria viene gestita attraverso i meccanismi di conservazione e recupero di un dataabase vettoriale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Memoria Lineare**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./linear_mem.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cronologia della conversazione attuale:\n",
      "MessageRole.USER: Ciao! Qual è il meteo oggi?\n",
      "MessageRole.ASSISTANT: Oggi è soleggiato con una temperatura di 25°C.\n",
      "MessageRole.USER: Perfetto, e domani?\n",
      "MessageRole.ASSISTANT: Domani è prevista pioggia con una temperatura di 18°C.\n",
      "\n",
      "Nuova cronologia della conversazione:\n",
      "MessageRole.USER: Ciao! Qual è il meteo oggi?\n",
      "MessageRole.ASSISTANT: Oggi è soleggiato con una temperatura di 25°C.\n",
      "MessageRole.USER: Perfetto, e domani?\n",
      "MessageRole.ASSISTANT: Domani è prevista pioggia con una temperatura di 18°C.\n",
      "MessageRole.USER: ('user', \"Grazie, allora prendo l'ombrello.\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "\n",
    "# Inizializza il buffer della memoria con una dimensione massima di 3 messaggi\n",
    "chat_memory = ChatMemoryBuffer(buffer_size=3, token_limit=3000)\n",
    "\n",
    "# Simula un'interazione utente-assistente\n",
    "chat_memory.put(ChatMessage(role=MessageRole.USER,      content=(\"Ciao! Qual è il meteo oggi?\")))\n",
    "chat_memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=(\"Oggi è soleggiato con una temperatura di 25°C.\")))\n",
    "chat_memory.put(ChatMessage(role=MessageRole.USER,      content=(\"Perfetto, e domani?\")))\n",
    "chat_memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=(\"Domani è prevista pioggia con una temperatura di 18°C.\")))\n",
    "\n",
    "\n",
    "# Recupera tutti i messaggi nel buffer\n",
    "chat_history = chat_memory.get()\n",
    "\n",
    "print(\"Cronologia della conversazione attuale:\")\n",
    "for message in chat_history:\n",
    "    print(f\"{message.role}: {message.content}\")\n",
    "\n",
    "# Aggiungi un nuovo messaggio che fa scattare la rimozione del messaggio più vecchio\n",
    "chat_memory.put(ChatMessage(role=MessageRole.USER,      content=((\"user\", \"Grazie, allora prendo l'ombrello.\"))))\n",
    "\n",
    "# Mostra la nuova cronologia con il messaggio più vecchio eliminato\n",
    "new_chat_history = chat_memory.get()\n",
    "print(\"\\nNuova cronologia della conversazione:\")\n",
    "for message in new_chat_history:\n",
    "    print(f\"{message.role}: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Memoria non lineare**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./non_linear_mem.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: <|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The conversation starts with the user asking for the weather forecast for today and tomorrow, which includes checking if it will be sunny or rainy. The assistant responds by providing the current temperature (25°C) and the forecasted conditions for both days: sunny with 18°C on the next day.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatSummaryMemoryBuffer\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "llm =  Ollama(model=\"llama3.2:1b\", \n",
    "                   temperature=0.5)\n",
    "\n",
    "\n",
    "memory_sum = ChatSummaryMemoryBuffer.from_defaults(\n",
    "    llm=llm,\n",
    "    chat_history=chat_memory.get(),\n",
    "    token_limit=2)\n",
    "\n",
    "history = memory_sum.get()\n",
    "for message in history:\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Memoria Peristente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./pers_mem.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
