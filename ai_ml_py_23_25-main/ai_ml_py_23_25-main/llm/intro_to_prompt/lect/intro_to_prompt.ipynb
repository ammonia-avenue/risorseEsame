{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ba6931",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dfd05",
   "metadata": {},
   "source": [
    "### Conda-Forge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d53d2",
   "metadata": {},
   "source": [
    "- **Installazione**\n",
    "\n",
    "https://conda-forge.org/\n",
    "\n",
    "In caso non si volesse avere l'enviroment già attivato nel terminale:\n",
    "```bash\n",
    "conda config --set auto_activate_base false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45d253",
   "metadata": {},
   "source": [
    "- **Creazione nuovo enviroment**\n",
    "\n",
    "    ```bash\n",
    "    conda create --name llm_env python=3.10\n",
    "    ```\n",
    "\n",
    "- **Attivazione enviroment**\n",
    "\n",
    "    ```bash\n",
    "    conda activate llama_env\n",
    "    ```\n",
    "\n",
    "- **Disattivazione enviroment**\n",
    "\n",
    "    ```bash\n",
    "    conda deactivate llama_env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a29436",
   "metadata": {},
   "source": [
    "### Installazione dipendenze progetto\n",
    "\n",
    "- **pip**\n",
    "\n",
    "    ```bash\n",
    "    conda install pip\n",
    "    ```\n",
    "\n",
    "\n",
    "- **[Llama Index](https://www.llamaindex.ai/)**\n",
    "\n",
    "    ```bash\n",
    "    pip install llama-index\n",
    "    ```\n",
    "\n",
    "    ```bash\n",
    "    pip install llama-index-llms-ollama\n",
    "\n",
    "    ```\n",
    "\n",
    "\n",
    "- **[Gradio](https://www.gradio.app/)**\n",
    "    ```bash\n",
    "    pip install gradio\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd77749",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Ollama](https://ollama.com/)\n",
    "\n",
    "### Install models\n",
    "\n",
    "```bash\n",
    "ollama pull <model:version>\n",
    "```\n",
    "\n",
    "**Models**\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "```bash\n",
    "# versione da usare per le lezioni\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b-instruct-q4_0\n",
    "```\n",
    "\n",
    "### Run models\n",
    "\n",
    "```bash\n",
    "ollama run <model:version>\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama serve <model:version>\n",
    "```\n",
    "\n",
    "### Remove models\n",
    "\n",
    "```bash\n",
    "ollama rm <model:version>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f19298",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967631a",
   "metadata": {},
   "source": [
    "## **Simple Chat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74388b6",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/api_reference/llms/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ab16ffbd3d1282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:59:42.932792Z",
     "start_time": "2024-11-11T13:59:42.930566Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac91c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_CHAT_TEMPLATE = \"\"\"Question: {question}\"\"\"\n",
    "\n",
    "def simple_chat(question: str = \"\", temperature: float = 0., top_p: float = 0.):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        @ question (string): passa il testo immesso da un utente al modell.\n",
    "\n",
    "        @ temperature (float): incide sulla capacità di un llm di scegliere l\n",
    "                               la successione di parole.\n",
    "\n",
    "    Return:\n",
    "       @ (string): risposta del modello linguistico.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    model = Ollama(model=\"llama3.2:1b\", \n",
    "                   temperature=temperature,\n",
    "                   top_p=top_p)\n",
    "\n",
    "\n",
    "    return model.complete(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6917ed8",
   "metadata": {},
   "source": [
    "## **Introduzione al Prompt Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd63263",
   "metadata": {},
   "source": [
    "Il Prompt Engineering consiste nella progettazione di testi specializzati (prompt) finalizzati a guidare i modelli di machine learning nella produzione di output accurati.\n",
    "Ogni risultato può essere condizionato da un prompt sulla base della loro lunghezza, struttura, ordine e rilevanza ai fini  dell'attività da svolgere.\n",
    "\n",
    "Volendo fare un elenco delle potenziali attività in cui viene impiegato:\n",
    "1. traduzioni\n",
    "2. gestione delle informazioni\n",
    "3. assistenza vocale\n",
    "4. generazione di codice\n",
    "5. sintesi di testi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ef513",
   "metadata": {},
   "source": [
    "### Parole Chiave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a735eb4",
   "metadata": {},
   "source": [
    "Gli llm come ChatGpt sono particolarmente reattivi all'uso di alcune paore chiave.\n",
    "\n",
    "\n",
    "|          |            |           |             |            |\n",
    "|:---:     |:---:       |:---:      |:---:        |:---:       |\n",
    "| Genera   | Sintetizza | Traduci   | Classifica  | Identifica |\n",
    "| Descrivi | Predici    | Consiglia | Analizza    | Valuta     |\n",
    "| Compara  | Contrasta  | Elenca    | Definisci   | Spiega     |\n",
    "| Estrai   | Trova      | Misura    | Categorizza | Seleziona  |\n",
    "| Ordina   | Recupera   | Raggruppa | Riconosci   | Organizza  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1e452",
   "metadata": {},
   "source": [
    "### Parametrizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db263ae8",
   "metadata": {},
   "source": [
    "Ogni llm si fonda sulla costruzione di una rete neurale. Questo complesso sistema matematcio non è direttamewnte accessibile in un modello pre-addestrato. Tuttavia è sempre possibile intervenire su alcuni parametri, in modo da influenzarne l'output.\n",
    "\n",
    "**Temperatura:** Determina il grado di casualità nel modo in cui un llm sceglie la successione di token.\n",
    "Più bassa è la temperatura, più deterministici sono i risultati. Mentre valori più alti (il range va da 0 a 1), aumentano la probabilità di avere composizioni di parole sempre diverse.\n",
    "Ne consegue che: per la generazione di documentazione è meglio avere una temperatura prossima allo zero. Invece se vogliamo ottenere testi più \"creativi\", conviene avere valori più vicini al massimo consentito.\n",
    "\n",
    "**Top_p:** Tecnica chiamata anche campionamento del nucleo.\n",
    "Se la temperatura sceglie la casalità con cui viene scelto il prosismo token,  questo metodo modifica la selezione dinamica del numero di parole da considerare per ogni fase delle previsioni del modello.\n",
    "Valori bassi portano a risultati più sicuri e più mirati. Valori più alti, permettono una selezione più ampia di parole, portando a risultati più diversificati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2f858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rose is indeed a beautiful and iconic flower. It's one of the most recognizable and beloved flowers in the world, known for its stunning appearance, delicate scent, and rich history. Roses have been cultivated for thousands of years, with evidence of their existence dating back to ancient civilizations in Egypt, Greece, and Rome.\n",
      "\n",
      "Roses come in a wide range of colors, each with its own unique characteristics and meanings. Red roses are often associated with love and romance, while pink roses symbolize appreciation and gratitude. White roses represent purity and innocence, while yellow roses signify friendship and joy.\n",
      "\n",
      "In addition to their beauty, roses have also played an important role in various cultures and traditions. In ancient Greece, roses were considered a symbol of Aphrodite, the goddess of love. In Victorian England, roses were used as a symbol of love and devotion during the era of Queen Victoria.\n",
      "\n",
      "Today, roses are still widely cultivated and enjoyed around the world, with many different varieties to choose from. Whether you're looking for a romantic gesture or simply want to add some beauty to your garden, there's no denying the allure of this stunning flower.\n"
     ]
    }
   ],
   "source": [
    "response = simple_chat(\"the rose is a flower\", temperature=0., top_p=0.)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1620056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecco la traduzione in italiano:\n",
      "\n",
      "La rosa è effettivamente una bellissima e iconica fiora. È uno dei più riconoscibili e amati fiori del mondo, noto per la sua impressionante apparizione, il suo profumo delicato e la sua storia ricca. Le rose sono state coltivate per migliaia di anni, con prove storiche dell'esistenza che risalgono alle antiche civiltà egizie, greche e rome.\n",
      "\n",
      "Le rose vengono in diverse colori, ciascuno con le sue caratteristiche uniche e significati. Le rose rosse sono spesso associate alla passione e all'amore, mentre le rose rosse simboleggiano l'apprezzamento e la gratitudine. Le rose bianche rappresentano purezza e innocenza, mentre le rose gialle significano amicizia e gioia.\n",
      "\n",
      "Inoltre, le rose hanno anche avuto un ruolo importante in molte culture e tradizioni. In Grecia antica, le rose erano considerate una simbologia dell'Afrodite, la dea della passione. In Inghilterra vittoriana, le rose furono utilizzate come simbolo di amore e dedizione durante l'epoca di Regina Victoria.\n",
      "\n",
      "Oggi, le rose sono ancora coltivate e apprezzate in tutto il mondo, con molte varietà da scegliere. Se cerchi un gesto romantico o vuoi aggiungere bellezza alla tua giardino, non puoi negare l'attrazione di questa bellissima fiora.\n",
      "\n",
      "Nota: ho mantenuto la struttura e il contenuto originale dell'articolo, ma ho utilizzato una versione più formale e lessicale italiana.\n"
     ]
    }
   ],
   "source": [
    "print(simple_chat(f\"translate in italian: {response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf07b38",
   "metadata": {},
   "source": [
    "Dall'esempio è evidente come un llm tenda sempre a completare il testo con una serie di parole cha hanno senso in base al contsto definito dal prompt.\n",
    "Minore è il numero di dettagli nel contenuto del testo, più grande sarà l'insieme di completamenti possibili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cf59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con forma de fiore e colori dolci, una delle più belle e simboliche del mondo, rappresenta l'amore.\n"
     ]
    }
   ],
   "source": [
    "print(simple_chat(\"Completa la frase: la rosa è un fiore \", temperature=1., top_p=1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08494e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La rosa è un fiore composto da diverse parti che lavorano insieme per produrre i fiori. Ecco una spiegazione scientifica:\n",
      "\n",
      "* La pianta rosa (Rosa spp.) è un albero erbaceo che produce fiori, foglie e frutta.\n",
      "* I fiori della rosa sono composti da petali, centrotipici e estremi. I petali sono le parti più grandi e sono disposti in cerchi circumpuntari intorno all'asse centrale del fiore.\n",
      "* I centrotipici sono le parti centrali dei petali, che si trovano tra i due poli di petali. Sono responsabili della formazione della pianta e dell'individuazione delle piante.\n",
      "* I estremi dei petali sono le parti più piccole e sono disposti in cerchi circumpuntari intorno all'asse centrale del fiore. Sono responsabili della produzione di olio essenziale e di altri composti chimici.\n",
      "\n",
      "La rosa è un fiore diofilo, cioè che produce i fiori attraverso una serie di processi biologici complessi. Ecco uno dei principali processi:\n",
      "\n",
      "* La pianta rosa inizia a produrre i fiori quando la temperatura del clima è sufficiente e l'umidità dell'aria è adatta.\n",
      "* La produzione dei fiori è influenzata dalla presenza di sostanze chimiche come il gibberellina, che regola la crescita della pianta.\n",
      "* La rosa produce i fiori attraverso una serie di processi biologici complessi, tra cui la formazione del centrotipico e l'individuazione delle piante.\n",
      "* I fiori della rosa sono sensibili alle sostanze chimiche come il luteolin, che regola la produzione di olio essenziale.\n",
      "\n",
      "In sintesi, la rosa è un fiore composto da diverse parti che lavorano insieme per produrre i fiori. La produzione dei fiori è influenzata dalla presenza di sostanze chimiche come il gibberellina e il luteolin, che regolano la crescita della pianta e la produzione di olio essenziale.\n"
     ]
    }
   ],
   "source": [
    "print(simple_chat(\"Completa la frase con una spiegazione scientifca: la rosa è un fiore \", temperature=0., top_p=0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae3956",
   "metadata": {},
   "source": [
    "### Componenti di un Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecae7b7",
   "metadata": {},
   "source": [
    "Come i linguaggi di programmazione hanno costrutti deputati ad astrarre determinati paradigmi, così il prompt engineering ha dei componenti finalizzati a sintetizzare le varie tipologie di istruzione.\n",
    "\n",
    "- **SYSTEM**: L'insieme delle parti di prompt.\n",
    "\n",
    "- **PROMPT GUARD**: Un'istruzione da inserire come prima istruzione per limitare le risposte del modello.\n",
    "\n",
    "- **CONTEXT**: Serve ad indirizzare il modello e contine l'insieme di informazioni utili a restringere e definire lo spazio all'interno del quale vengono selezionati i token.\n",
    "\n",
    "- **INSTRUCTION**: Indica il compito specifico che il modello deve seguire. In generale riguarda una ***keyword*** da inserire ad inizio prompt e che determina la finalità stessa di tutto il prompt.\n",
    "\n",
    "\n",
    "- **INPUT**: La frase immessa dall'utente su cui orientare l'output. Può esserlo ad esempio, una domanda per cui si vuole ottenere una risposta più o meno precisa.\n",
    "\n",
    "\n",
    "- **OUTPUT**: Definisce il modo in cui il modello deve restituire l'output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eed935",
   "metadata": {},
   "source": [
    "Ogni Language Model ha il suo ***manuale di stile***\n",
    "\n",
    "- [OpenAi](https://www.llama.com/docs/how-to-guides/prompting/)\n",
    "\n",
    "- [LLama](https://www.llama.com/docs/how-to-guides/prompting/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd7f33",
   "metadata": {},
   "source": [
    "| N.B: In generale l'inglese è il linguaggio che performa meglio nel prompt engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
