{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ba6931",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dfd05",
   "metadata": {},
   "source": [
    "### Conda-Forge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d53d2",
   "metadata": {},
   "source": [
    "- **Installazione**\n",
    "\n",
    "https://conda-forge.org/\n",
    "\n",
    "In caso non si volesse avere l'enviroment già attivato nel terminale:\n",
    "```bash\n",
    "conda config --set auto_activate_base false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45d253",
   "metadata": {},
   "source": [
    "- **Creazione nuovo enviroment**\n",
    "\n",
    "    ```bash\n",
    "    conda create --name llm_env python=3.10\n",
    "    ```\n",
    "\n",
    "- **Attivazione enviroment**\n",
    "\n",
    "    ```bash\n",
    "    conda activate llama_env\n",
    "    ```\n",
    "\n",
    "- **Disattivazione enviroment**\n",
    "\n",
    "    ```bash\n",
    "    conda deactivate llama_env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a29436",
   "metadata": {},
   "source": [
    "### Installazione dipendenze progetto\n",
    "\n",
    "- **pip**\n",
    "\n",
    "    ```bash\n",
    "    conda install pip\n",
    "    ```\n",
    "\n",
    "\n",
    "- **[Llama Index](https://www.llamaindex.ai/)**\n",
    "\n",
    "    ```bash\n",
    "    pip install llama-index\n",
    "    ```\n",
    "\n",
    "    ```bash\n",
    "    pip install llama-index-llms-ollama\n",
    "\n",
    "    ```\n",
    "\n",
    "\n",
    "- **[Gradio](https://www.gradio.app/)**\n",
    "    ```bash\n",
    "    pip install gradio\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd77749",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Ollama](https://ollama.com/)\n",
    "\n",
    "### Install models\n",
    "\n",
    "```bash\n",
    "ollama pull <model:version>\n",
    "```\n",
    "\n",
    "**Models**\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "```\n",
    "\n",
    "```bash\n",
    "# versione da usare per provare gli agenti e i tool\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "```bash\n",
    "# versione da usare per le lezioni\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b-instruct-q4_0\n",
    "```\n",
    "\n",
    "### Run models\n",
    "\n",
    "```bash\n",
    "ollama run <model:version>\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama serve <model:version>\n",
    "```\n",
    "\n",
    "### Remove models\n",
    "\n",
    "```bash\n",
    "ollama rm <model:version>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f19298",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967631a",
   "metadata": {},
   "source": [
    "## **Simple Chat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74388b6",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/api_reference/llms/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ab16ffbd3d1282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:59:42.932792Z",
     "start_time": "2024-11-11T13:59:42.930566Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac91c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_chat(question: str = \"\", temperature: float = 0., top_p: float = 0.):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        @ question (string): passa il testo immesso da un utente al modell.\n",
    "\n",
    "        @ temperature (float): incide sulla capacità di un llm di scegliere l\n",
    "                               la successione di parole.\n",
    "\n",
    "    Return:\n",
    "       @ (string): risposta del modello linguistico.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    model = Ollama(model=\"llama3.2:1b\", \n",
    "                   temperature=temperature,\n",
    "                   top_p=top_p)\n",
    "\n",
    "\n",
    "    return model.complete(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351defa",
   "metadata": {},
   "source": [
    "# **Tecniche di Prompt Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e33be",
   "metadata": {},
   "source": [
    "Le tecniche di prompt engineering si concentrano su vari aspetti, come la formulazione chiara e concisa della domanda, l'inclusione di contesti specifici, la strutturazione del prompt in modo che il modello comprenda correttamente l'intento dell'utente, e l'uso di parametri come la temperatura o la lunghezza della risposta. Un buon prompt può migliorare significativamente la qualità delle risposte ottenute, mentre un prompt mal strutturato può portare a risultati imprecisi o incoerenti.\n",
    "\n",
    "Le principali tecniche di prompt engineering includono:\n",
    "\n",
    "- **Prompt chiari e diretti**: Evitare ambiguità e assicurarsi che la richiesta sia comprensibile.\n",
    "- **Contesto e dettagli**: Fornire informazioni aggiuntive per indirizzare il modello verso risposte più pertinenti.\n",
    "- **Format specifico**: Utilizzare istruzioni che richiedano una risposta in un formato definito, come liste o paragrafi.\n",
    "- **Temperature e altre variabili**: Regolare parametri come la temperatura per influenzare la creatività o la precisione delle risposte.\n",
    "- **Prompt iterativi**: Affinare il prompt attraverso tentativi successivi, per affinare la risposta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a7408",
   "metadata": {},
   "source": [
    "## **L'uso della negazione**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d9191",
   "metadata": {},
   "source": [
    "Non sembra essere una buona pratica, quella di scrivere esplicitamente cosa ***NON deve fare un LLM***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf962590",
   "metadata": {},
   "source": [
    "```\n",
    "Don't say ... \n",
    "Don't do ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f2f9b",
   "metadata": {},
   "source": [
    "Le espressioni legate alla negazione esplictia possono essere legate a contesti e finalità differenti. \n",
    "L'effetto finale, potrebbe quindi essere quello di determinare una serie di ambiguità nella definizione dei pattern all'interno di un llm, tali da aumentare la probabilità di risultati poco coerenti.\n",
    "\n",
    "\n",
    "```\n",
    "Don't use these words: ... \n",
    "```\n",
    "\n",
    "```\n",
    "You don't have this type of item at your disposal: ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc936761",
   "metadata": {},
   "source": [
    "Usare in alternativa espressioni univoche,  accompagnate da una spiegazione.\n",
    "\n",
    "```\n",
    "Avoid saying these words: ...\n",
    "```\n",
    "\n",
    "La stessa forma, utilizzata per esprimere una quantità non avrebbe senso o per dirla come un llm:\n",
    "la probabilità che sia presente nel dataset di addestramento, è molto poco probabile.\n",
    "\n",
    "```\n",
    "Avoid having these items available: ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac403af",
   "metadata": {},
   "source": [
    "N.B | Quanto detto vale non solo per il prompt engineering, ma anche nell'uso quotidiano di un llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4079ad7",
   "metadata": {},
   "source": [
    "## **In-Context Learning (ICL)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e9f62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Si tratta di un concetto centrale nei modelli linguistici avanzati, come GPT,  si riferisce alla capacità del modello di apprendere e adattarsi a un compito specifico fornendo esempi o istruzioni direttamente nel prompt, senza dover essere ulteriormente addestrato o fine-tuned.\n",
    "\n",
    "\n",
    "- **Uso del contesto**\n",
    "\n",
    "Il modello utilizza il testo fornito nel prompt come guida per generare risposte o completare un compito. Gli esempi servono come \"istruzioni implicite\" per definire il comportamento atteso.\n",
    "\n",
    "- **Formati di prompting**\n",
    "\n",
    "    - **Zero-shot**: Nessun esempio, solo istruzioni.\n",
    "    - **One-shot**: Un esempio per chiarire il formato o il compito.\n",
    "    - **Few-shot**: Più esempi per fornire un contesto più completo.\n",
    "    \n",
    "\n",
    "- **Adattabilità senza riaddestramento**\n",
    "\n",
    "A differenza dei tradizionali metodi di apprendimento, in cui il modello viene addestrato con nuovi dati, l'ICL sfrutta la conoscenza preesistente del modello, combinandola con le informazioni fornite nel prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbd620",
   "metadata": {},
   "source": [
    "### **Zero-shot prompting**\n",
    "\n",
    "Non si fornisce alcun esempio al modello. Gli si chiede semplicemente di rispondere a una domanda o completare un compito basandosi solo sulla comprensione implicita del linguaggio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44073b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would classify the text as neutral. The user's statement \"I think the vacation is okay\" does not express a strong emotion such as positivity or negativity. It simply states a factual opinion without any emotional tone.\n"
     ]
    }
   ],
   "source": [
    "ZERO_SHOT = \\\n",
    "\"\"\"\n",
    "<INSTRUCTION>\n",
    "Classify the text into neutral, negative or positive. \n",
    "\n",
    "<CHAT>\n",
    "User: I think the vacation is okay.\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "print(simple_chat(ZERO_SHOT, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82960c64",
   "metadata": {},
   "source": [
    "### **One/Few-shot prompting**\n",
    "\n",
    "\n",
    "Si forniscono alcuni esempi (1-5) per rafforzare la comprensione del modello rispetto alla natura del compito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7f7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can classify the text as follows:\n",
      "\n",
      "- User: \"What a horrible show!\" - Neutral (the user's opinion is subjective and not necessarily positive or negative)\n",
      "- Assistant: \"This is bad!\" - Negative (the assistant's response is direct and expresses disapproval)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ONE_SHOT = \\\n",
    "\"\"\"\n",
    "<INSTRUCTION>\n",
    "Classify the text into neutral, negative or positive. \n",
    "\n",
    "<EXAMPLES>\n",
    "User: This is awesome! \n",
    "Assistant: Positive \n",
    "User: This is bad! \n",
    "Assistant: Negative\n",
    "User: Wow that movie was rad!\n",
    "Assistatn: Positive\n",
    "\n",
    "\n",
    "<CHAT>\n",
    "User: What a horrible show! \n",
    "Answare: \n",
    "\"\"\"\n",
    "\n",
    "print(simple_chat(ONE_SHOT, 0., 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7d475",
   "metadata": {},
   "source": [
    "## **Chain-of-Thought (CoT) Prompt**\n",
    "\n",
    "[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e528976",
   "metadata": {},
   "source": [
    "La tecnica di ***Chain-of-Thought (CoT)**** è un metodo di prompting progettato per migliorare le capacità di ragionamento dei modelli linguistici. Consiste nel guidare il modello attraverso una sequenza esplicita di passaggi intermedi per risolvere un problema complesso, anziché richiedere direttamente la risposta finale. Questo approccio incoraggia il modello a \"pensare passo dopo passo\" e a simulare un processo di ragionamento simile a quello umano.\n",
    "\n",
    "La maggior parte dei modelli attuali è stato già addestrato per usare questa tecnica in autonomia. \n",
    "Se l'uso di questo approccio non è contenuto in modo esplicito dal contesto, può essere \"attivato\" \n",
    "da espressioni come:\n",
    "\n",
    "```\n",
    "proceed step by step\n",
    "```\n",
    "\n",
    "Questa espressione, utilizzata direttamente, permette anche di comprendere in che modo costruire un \"ragionamento\" esplicito, a partire dal tipo di output prodotto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37672a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how long each individual piece of wire is, we need to divide the total length of the wire (10 cm) by the number of pieces it was cut into (6).\n",
      "\n",
      "10 cm ÷ 6 = 1.67 cm per piece\n",
      "\n",
      "So each individual piece of wire is approximately 1.67 cm long.\n"
     ]
    }
   ],
   "source": [
    "BASE_PROMPT = \\\n",
    "\"\"\"\n",
    "Tom used a pice of wire 10cm long to support tomato plants in the garden.\n",
    "The wire was cut into 6 pices. \n",
    "How long is each individual piece of wire?\n",
    "\"\"\"\n",
    "\n",
    "print(simple_chat(BASE_PROMPT, 0., 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d43342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how long each individual piece of wire is, we need to divide the total length of the wire by the number of pieces it was cut into.\n",
      "\n",
      "Step 1: Find the total length of the wire\n",
      "The total length of the wire is given as 10cm.\n",
      "\n",
      "Step 2: Divide the total length by the number of pieces\n",
      "To find out how long each individual piece is, we divide the total length (10cm) by the number of pieces it was cut into (6).\n",
      "\n",
      "10 cm ÷ 6 = 1.67 cm\n",
      "\n",
      "So each individual piece of wire is approximately 1.67cm long.\n"
     ]
    }
   ],
   "source": [
    "SBS_PROMPT = \\\n",
    "BASE_PROMPT + \"Proceed step-by-step\"\n",
    "\n",
    "print(simple_chat(SBS_PROMPT, 0., 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c670d",
   "metadata": {},
   "source": [
    "## **ReAct** (Reason + Act).\n",
    "\n",
    "[Yao et al.2022](https://arxiv.org/pdf/2210.03629.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f26aa2",
   "metadata": {},
   "source": [
    "La tecnica di ReAct (Reasoning + Acting) combina ragionamento logico e azioni interattive nei prompt, per permettere ai modelli linguistici di risolvere problemi complessi che richiedono sia pensiero deduttivo che interazioni con l'ambiente (ad esempio, consultare un database o chiamare una funzione in un determinato linguaggio di programmazione)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3dca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divides two integers and returns the result integer\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "\n",
    "def test_agent():\n",
    "\n",
    "\n",
    "    llm = Ollama(model=\"llama3.2:1b\")\n",
    "    #llm = Ollama(model=\"llama3.2:3b\")\n",
    "    #llm = Ollama(model=\"llama3.1:8b\")\n",
    "    \n",
    "\n",
    "    multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "    add_tool = FunctionTool.from_defaults(fn=add)\n",
    "    subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "    divide_tool = FunctionTool.from_defaults(fn=divide)\n",
    "\n",
    "\n",
    "    agent = ReActAgent.from_tools(\n",
    "        [multiply_tool, add_tool, subtract_tool, divide_tool],\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    response = agent.chat(\"What is (121 + 2) * 5?\")\n",
    "    print(str(response))\n",
    "\n",
    "test_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
